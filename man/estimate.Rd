% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/estimate.R
\name{estimate}
\alias{estimate}
\title{GGM: Estimation}
\usage{
estimate(
  Y,
  formula = NULL,
  data = NULL,
  type = "continuous",
  mixed_type = NULL,
  iter = 5000,
  analytic = FALSE,
  ep = 0.001,
  ...
)
}
\arguments{
\item{Y}{matrix (or data frame) of dimensions \emph{n} (observations) by  \emph{p} (variables).}

\item{formula}{an object of class \code{\link[stats]{formula}}. This allows for including
control variables in the model (i.e., \code{~ gender}).}

\item{data}{an optional data frame, list or environment (or an object coercible by \code{\link[base]{as.data.frame}})
to a data frame containing the variables in \code{formula}. This is required when controlling for variables.}

\item{type}{character string. Which type of data for \strong{Y} ? The options include \code{continuous},
\code{binary}, \code{ordinal}, or \code{mixed}. See the note for further details.}

\item{mixed_type}{numeric vector. An indicator of length p for which varibles should be treated as ranks.
(1 for rank and 0 to assume normality). The default is currently to treat all integer variables as ranks
when \code{type = "mixed"} and \code{NULL} otherwise. See note for further details.

Treating continous data as ranks is computationally expensive and can be avoided by assuming normality.}

\item{iter}{number of iterations (posterior samples; defaults to 5000).}

\item{analytic}{logical. Should the analytic solution be computed (default is \code{FALSE})?}

\item{...}{currently ignored.}
}
\value{
list of class \code{estimate}:

\code{analytic = TRUE}:
\itemize{
\item \code{fit} list of analytic solution estimates
\itemize{
\item \code{inv_mu} inverse covariance matrix (mean)
\item \code{inv_var} inverse covariance matrix (variance)
\item \code{partial} partial correlation matrix
}
\item \code{analytic} TRUE
\item \code{call} match.call()
\item \code{dat} data matrix
\item \code{p} number of variables
}

\code{analytic = FALSE}:
\itemize{
\item \code{parcors_mat} partial correlation matrix
\item \code{inv_mat} inverse covariance matrix
\item \code{posterior samples} posterior samples for partial correlations and inverse covariance matrix
\item \code{p} number of variables
\item \code{dat} data matrix
\item \code{iter} number of posterior samples
\item \code{call} match.call()
\item \code{analytic} FALSE
}
}
\description{
Estimate the conditional (in)dependence with either an analytic solution or efficiently
sampling from the posterior distribution. These methods were introduced in \insertCite{Williams2019;textual}{BGGM}.
The graph is then selected with \code{\link{select.estimate}}, with either directional posterior probabilities
\insertCite{Marsman2017a}{BGGM}, credible intervals, or a region of practical equivalence \insertCite{Kruschke2017}{BGGM}.
Bayesian hypothesis testing is implemented in \code{\link{explore}} and \code{\link{confirm}} \insertCite{Williams2019_bf}{BGGM}.
}
\note{
The default is to draws samples from the posterior distribution (\code{analytic = FALSE}). The samples are
required for computing edge differences (see \code{\link{ggm_compare_estimate}}), Bayesian R2 introduced in
 \insertCite{gelman_r2_2019;textual}{BGGM} (see \code{\link{bayes_R2}}), etc. If the goal is to *only* determine the non-zero effects, this can be accomplished by setting \code{analytic = TRUE}. Note also sampling is
very fast--i.e., less than 1 second with p = 25, n = 2500 and 5,000 samples.

These methods are inherently Bayesian. This also means there is a close correspondence to "frequentist" methods or, say,
maximum likelihood. In fact, the prior distribution is set to mimic the unbiased estimate of the sample based
covariance matrix. This allows for efficiently drawing samples from the posterior distribution. The advantage compared
to frequentist methods is that a measure of uncertainty is readily available. This allows for
seamlessly computing partial correlation (see \code{\link{ggm_compare_estimate}}) and Bayesian R2  (see \code{\link{test.R2}}) differences.
Further, the posterior probability of a null region (see \code{\link{select.estimate}}) can be computed and this provides
an estimate of the conditional independence structure (null effects).


\strong{Interpretation of conditional (in)dependence models for latent data:}

A  tetrachoric correlation (binary data) is a special case of a polychoric correlation (ordinal data). Both relations are
between "theorized normally distributed continuous latent variables"
(\href{https://en.wikipedia.org/wiki/Polychoric_correlation}{Wikipedia})
In both instances, the correpsonding partial correlation between observed variables is conditioned
on the remaining variables in the \emph{latent} space. This implies that interpration is much the same as
for continuous data, but with respect to latent variables. We refer interested reader to
\insertCite{@page 2364, section 2.2, in  @webb2008bayesian;textual}{BGGM}.


see \code{methods("estimate")}
}
\examples{
# p = 20
Y <- BGGM::bfi[, 1:5]

# analytic approach (sample by setting analytic = FALSE)
fit_analytic <- estimate(Y, analytic = TRUE)

# select the graph (edge set E)
E <- select(fit_analytic, ci_width = 0.95)

}
\references{
\insertAllCited{}
}
