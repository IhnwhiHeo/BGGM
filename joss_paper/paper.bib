@article{Epskamp2018,
abstract = {We discuss the Gaussian graphical model (GGM; an undirected network of partial correlation coefficients) and detail its utility as an exploratory data analysis tool. The GGM shows which variables predict one-another, allows for sparse modeling of covariance structures, and may highlight potential causal relationships between observed variables. We describe the utility in 3 kinds of psychological datasets: datasets in which consecutive cases are assumed independent (e.g., cross-sectional data), temporally ordered datasets (e.g., n = 1 time series), and a mixture of the 2 (e.g., n {\textgreater} 1 time series). In time-series analysis, the GGM can be used to model the residual structure of a vector-autoregression analysis (VAR), also termed graphical VAR. Two network models can then be obtained: a temporal network and a contemporaneous network. When analyzing data from multiple subjects, a GGM can also be formed on the covariance structure of stationary means---the between-subjects network. We discuss the interpretation of these models and propose estimation methods to obtain these networks, which we implement in the R packages graphicalVAR and mlVAR. The methods are showcased in two empirical examples, and simulation studies on these methods are included in the supplementary materials.},
archivePrefix = {arXiv},
arxivId = {1609.04156},
author = {Epskamp, Sacha and Waldorp, Lourens J. and Mottus, Rene and Borsboom, Denny},
doi = {10.1080/00273171.2018.1454823},
eprint = {1609.04156},
journal = {Multivariate Behavioral Research},
keywords = {Time-series analysis,exploratory-data analysis,multilevel modeling,multivariate analysis,network modeling},
month = {jul},
number = {4},
pages = {453--480},
publisher = {Routledge},
title = {{The Gaussian Graphical Model in Cross-Sectional and Time-Series Data}},
volume = {53},
year = {2018}
}

@article{Schafer2005,
abstract = {Inferring large-scale covariance matrices from sparse genomic data is an ubiquitous problem in bioinformatics. Clearly, the widely used standard covariance and correlation estimators are ill-suited for this purpose. As statistically efficient and computationally fast alternative we propose a novel shrinkage covariance estimator that exploits the Ledoit-Wolf (2003) lemma for analytic calculation of the optimal shrinkage intensity.Subsequently, we apply this improved covariance estimator (which has guaranteed minimum mean squared error, is well-conditioned, and is always positive definite even for small sample sizes) to the problem of inferring large-scale gene association networks. We show that it performs very favorably compared to competing approaches both in simulations as well as in application to real expression data.},
author = {Sch{\"{a}}fer, Juliane and Strimmer, Korbinian},
doi = {10.2202/1544-6115.1175},
file = {:C$\backslash$:/Users/willidon/Downloads/[Statistical Applications in Genetics and Molecular Biology] A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics.pdf:pdf},
isbn = {1544-6115},
issn = {1544-6115},
journal = {Statistical Applications in Genetics and Molecular Biology},
mendeley-groups = {cov{\_}inv{\_}matrix},
number = {1},
pmid = {16646851},
title = {{A Shrinkage Approach to Large-Scale Covariance Matrix Estimation and Implications for Functional Genomics}},
volume = {4},
year = {2005}
}

@article{Das2017,
abstract = {The correlation method from brain imaging has been used to estimate functional connectivity in the human brain. However, brain regions might show very high correlation even when the two regions are not directly connected due to the strong interaction of the two regions with common input from a third region. One previously proposed solution to this problem is to use a sparse regularized inverse covariance matrix or precision matrix (SRPM) assuming that the connectivity structure is sparse. This method yields partial correlations to measure strong direct interactions between pairs of regions while simultaneously removing the influence of the rest of the regions, thus identifying regions that are conditionally independent. To test our methods, we first demonstrated conditions under which the SRPM method could indeed find the true physical connection between a pair of nodes for a spring-mass example and an RC circuit example. The recovery of the connectivity structure using the SRPM method can be explained by energy models using the Boltzmann distribution. We then demonstrated the application of the SRPM method for estimating brain connectivity during stage 2 sleep spindles from human electrocorticography (ECoG) recordings using an [Formula: see text] electrode array. The ECoG recordings that we analyzed were from a 32-year-old male patient with long-standing pharmaco-resistant left temporal lobe complex partial epilepsy. Sleep spindles were automatically detected using delay differential analysis and then analyzed with SRPM and the Louvain method for community detection. We found spatially localized brain networks within and between neighboring cortical areas during spindles, in contrast to the case when sleep spindles were not present.},
archivePrefix = {arXiv},
arxivId = {1309.2848v1},
author = {Das, A and Sampson, AL and Lainscsek, C and Muller, L and Lin, W and Doyle, JC and Cash, SS and Halgren, E and Sejnowski, TJ},
doi = {10.1162/NECO_a_00936},
eprint = {1309.2848v1},
file = {:C$\backslash$:/Users/willidon/Downloads/nihms856787.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
mendeley-groups = {cov{\_}inv{\_}matrix},
number = {3},
pages = {603--642},
pmid = {25602775},
title = {{Interpretation of the Precision Matrix and Its Application in Estimating Sparse Brain Connectivity during Sleep Spindles from Human Electrocorticography Recordings}},
volume = {29},
year = {2017}
}
@article{Friedman2008,
abstract = {We consider the problem of estimating sparse graphs by a lasso penalty applied to the inverse covariance matrix. Using a coordinate descent procedure for the lasso, we develop a simple algorithm--the graphical lasso--that is remarkably fast: It solves a 1000-node problem ( approximately 500,000 parameters) in at most a minute and is 30-4000 times faster than competing methods. It also provides a conceptual link between the exact problem and the approximation suggested by Meinshausen and B{\"{u}}hlmann (2006). We illustrate the method on some cell-signaling data from proteomics.},
archivePrefix = {arXiv},
arxivId = {0708.3517},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1093/biostatistics/kxm045},
eprint = {0708.3517},
file = {:C$\backslash$:/Users/willidon/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman, Hastie, Tibshirani - 2008 - Sparse inverse covariance estimation with the graphical lasso.pdf:pdf},
isbn = {1468-4357 (Electronic)$\backslash$n1465-4644 (Linking)},
issn = {14654644},
journal = {Biostatistics},
keywords = {Gaussian covariance,Graphical model,L1,Lasso},
mendeley-groups = {cov{\_}inv{\_}matrix},
month = {jul},
number = {3},
pages = {432--441},
pmid = {18079126},
publisher = {Oxford University Press},
title = {{Sparse inverse covariance estimation with the graphical lasso}},
volume = {9},
year = {2008}
}
@article{Williams2018bayes,
author = {Williams, Donald R},
doi = {10.31234/OSF.IO/X8DPR},
file = {:C$\backslash$:/Users/willidon/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams - 2018 - Bayesian Inference for Gaussian Graphical Models Structure Learning, Explanation, and Prediction.pdf:pdf},
mendeley-groups = {general,cov{\_}inv{\_}matrix},
publisher = {PsyArXiv},
title = {{Bayesian Inference for Gaussian Graphical Models: Structure Learning, Explanation, and Prediction}},
year = {2018}
}


@article{Leday2018,
abstract = {Despite major methodological developments, Bayesian inference for Gaussian graphical models remains challenging in high dimension due to the tremendous size of the model space. This article proposes a method to infer the marginal and conditional independence structures between variables by multiple testing of hypotheses. Specifically, we introduce closed-form Bayes factors under the Gaussian conjugate model to evaluate the null hypotheses of marginal and conditional independence between variables. Their computation for all pairs of variables is shown to be extremely efficient, thereby allowing us to address large problems with thousands of nodes. Moreover, we derive exact tail probabilities from the null distributions of the Bayes factors. These allow the use of any multiplicity correction procedure to control error rates for incorrect edge inclusion. We demonstrate the proposed approach to graphical model selection on various simulated examples as well as on a large gene expression data set from The Cancer Genome Atlas.},
archivePrefix = {arXiv},
arxivId = {1803.08155},
author = {Leday, Gwena{\"{e}}l G. R. and Richardson, Sylvia},
eprint = {1803.08155},
file = {:C$\backslash$:/Users/willidon/Box Sync/latex/1803.08155.pdf:pdf},
mendeley-groups = {bayes{\_}boot,cov{\_}inv{\_}matrix},
pages = {1--26},
title = {{Fast Bayesian inference in large Gaussian graphical models}},
year = {2018}
}


@article{Mohammadi2015a,
abstract = {Decoding complex relationships among large numbers of variables with relatively few observations is one of the crucial issues in science. One approach to this problem is Gaussian graphical modeling, which describes conditional in-dependence of variables through the presence or absence of edges in the underly-ing graph. In this paper, we introduce a novel and efficient Bayesian framework for Gaussian graphical model determination which is a trans-dimensional Markov Chain Monte Carlo (MCMC) approach based on a continuous-time birth-death process. We cover the theory and computational details of the method. It is easy to implement and computationally feasible for high-dimensional graphs. We show our method outperforms alternative Bayesian approaches in terms of convergence, mixing in the graph space and computing time. Unlike frequentist approaches, it gives a principled and, in practice, sensible approach for structure learning. We il-lustrate the efficiency of the method on a broad range of simulated data. We then apply the method on large-scale real applications from human and mammary gland gene expression studies to show its empirical usefulness. In addition, we implemented the method in the R package BDgraph which is freely available at http://CRAN.R-project.org/package=BDgraph.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.5371v6},
author = {Mohammadi, A. and Wit, E. C.},
doi = {10.1214/14-BA889},
eprint = {arXiv:1210.5371v6},
file = {:C$\backslash$:/Users/willidon/Downloads/euclid.ba.1422468425.pdf:pdf},
isbn = {0884-0431 (Print)},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Bayesian model selection,Birth-death process,G-Wishart,Markov chain Monte Carlo,Non-decomposable graphs,Sparse Gaussian graphical models},
mendeley-groups = {cov{\_}inv{\_}matrix},
number = {1},
pages = {109--138},
pmid = {14969389},
title = {{Bayesian structure learning in sparse Gaussian graphical models}},
volume = {10},
year = {2015}
}

@article{Kuismin2017,
author = {Kuismin, Markku and Sillanp{\"{a}}{\"{a}}, Mikko},
doi = {10.1002/wics.1415},
file = {:C$\backslash$:/Users/willidon/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuismin, Sillanp{\"{a}}{\"{a}} - 2017 - Estimation of covariance and precision matrix, network structure, and a view toward systems biology.pdf:pdf},
issn = {19390068},
journal = {Wiley Interdisciplinary Reviews: Computational Statistics},
keywords = {Gaussian graphical models,Sparse precision matrix,covariance matrix,microarray data,network estimation,shrinkage estimation},
mendeley-groups = {cov{\_}inv{\_}matrix},
number = {6},
pages = {1--13},
title = {{Estimation of covariance and precision matrix, network structure, and a view toward systems biology}},
volume = {9},
year = {2017}
}


@article{Haslbeck2018,
abstract = {Network models are an increasingly popular way to abstract complex psychological phenomena. While the study of the structure of network models has led to many important insights, little attention is paid to how well they predict observations. This is despite the fact that predictability is crucial for judging the practical relevance of edges: for instance in clinical practice, predictability of a symptom indicates whether a an intervention on that symptom through the symptom network is promising. We close this methodological gap by introducing nodewise predictability, which quantifies how well a given node can be predicted by all other nodes it is connected to in the network. In addition, we provide fully reproducible code examples of how to compute and visualize nodewise predictability both for cross-sectional and time-series data.},
archivePrefix = {arXiv},
arxivId = {1610.09108},
author = {Haslbeck, Jonas M B and Waldorp, Lourens J.},
doi = {10.3758/s13428-017-0910-x},
eprint = {1610.09108},
file = {:C$\backslash$:/Users/willidon/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haslbeck, Waldorp - 2018 - How well do network models predict observations On the importance of predictability in network models.pdf:pdf},
isbn = {1342801709},
issn = {15543528},
journal = {Behavior Research Methods},
keywords = {Clinical relevance,Network analysis,Network models,Predictability},
mendeley-groups = {mbr{\_}special},
month = {apr},
number = {2},
pages = {853--861},
pmid = {28718088},
publisher = {Springer US},
title = {{How well do network models predict observations? On the importance of predictability in network models}},
volume = {50},
year = {2018}
}

@article{Mulder2018,
abstract = {The matrix-F distribution is presented as prior for covariance matrices as an alternative to the conjugate inverted Wishart distribution. A special case of the univariate F distribution for a variance parameter is equivalent to a half-t distribution for a standard deviation, which is becoming increasingly popular in the Bayesian literature. The matrix-F distribution can be conveniently modeled as a Wishart mixture of Wishart or inverse Wishart distributions, which allows straightforward implementation in a Gibbs sampler. By mixing the covari-ance matrix of a multivariate normal distribution with a matrix-F distribution, a multivariate horseshoe type prior is obtained which is useful for modeling sparse signals. Furthermore, it is shown that the intrinsic prior for testing covariance matrices in non-hierarchical models has a matrix-F distribution. This intrinsic prior is also useful for testing inequality constrained hypotheses on variances. Finally through simulation it is shown that the matrix-variate F distribution has good frequentist properties as prior for the random effects covariance matrix in generalized linear mixed models.},
author = {Mulder, Joris and Pericchi, Luis},
doi = {10.1214/17-BA1092},
file = {:C$\backslash$:/Users/willidon/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mulder, Ra{\'{u}}l Pericchi - 2018 - The Matrix-F Prior for Estimating and Testing Covariance Matrices.pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {hierarchical models,horsehoe prior,intrinsic prior,matrix-variate F distribution,testing inequality constraints},
mendeley-groups = {cov{\_}inv{\_}matrix},
number = {4},
pages = {1--22},
title = {{The Matrix-F Prior for Estimating and Testing Covariance Matrices}},
year = {2018}
}

@article{gelman_r2_2019,
abstract = {The usual definition of R2 (variance of the predicted values divided by the variance of the data) has a problem for Bayesian fits, as the numerator can be larger than the denominator. We propose an alternative definition similar to one that has appeared in the survival analysis literature: the variance of the predicted values divided by the variance of predicted values plus the expected variance of the errors.},
author = {Gelman, Andrew and Goodrich, Ben and Gabry, Jonah and Vehtari, Aki},
booktitle = {American Statistician},
doi = {10.1080/00031305.2018.1549100},
issn = {15372731},
keywords = {Bayesian methods,R-squared,Regression},
month = {jul},
number = {3},
pages = {307--309},
publisher = {American Statistical Association},
title = {{R-squared for Bayesian Regression Models}},
volume = {73},
year = {2019}
}


@article{williams_ggmcompare,
author = {Williams, Donald R and Rast, Philippe and Pericchi, Luis R and Mulder, Joris},
doi = {https://doi.org/10.31234/osf.io/yt386},
file = {:C$\backslash$:/Users/willidon/Downloads/pre{\_}print (20).pdf:pdf},
keywords = {bayes factor,gaussian graphical model,partial,posterior predictive distribution},
title = {{Comparing Gaussian Graphical Models with the Posterior Predictive Distribution and Bayesian Model Selection}},
year = {2019}
}


@article{brigantiresilience,
author = {Briganti, G and Linkowski, P},
doi = {10.1017/S2045796019000222},
issn = {2045-7960, 2045-7979},
journal = {Epidemiology and Psychiatric Sciences},
keywords = {Behaviour,community detection algorithms,psychometrics},
pages = {1--9},
title = {{Item and domain network structures of the Resilience Scale for Adults in 675 university students}},
year = {2019}
}


@article{williams_bayesian_2019,
author = {Williams, Donald R and Mulder, Joris},
doi = {10.31234/osf.io/ypxd8},
journal = {PsyArXiv},
month = {jan},
title = {{Bayesian Hypothesis Testing for Gaussian Graphical Models: Conditional Independence and Order Constraints}},
year = {2019}
}


@Article{bayesplot,
    title = {Visualization in Bayesian workflow},
    author = {Jonah Gabry and Daniel Simpson and Aki Vehtari and Michael Betancourt and Andrew Gelman},
    year = {2019},
    journal = {J. R. Stat. Soc. A},
    volume = {182},
    issue = {2},
    pages = {389-402},
    doi = {10.1111/rssa.12378},
  }
  
  
 @Manual{ggridges,
    title = {ggridges: Ridgeline Plots in 'ggplot2'},
    author = {Claus O. Wilke},
    year = {2018},
    note = {R package version 0.5.1},
    url = {https://CRAN.R-project.org/package=ggridges},
  }
  
  
  @Book{ggplot,
    author = {Hadley Wickham},
    title = {ggplot2: Elegant Graphics for Data Analysis},
    publisher = {Springer-Verlag New York},
    year = {2016},
    isbn = {978-3-319-24277-4},
    url = {https://ggplot2.tidyverse.org},
  }
  
  @article{Stephens2018,
abstract = {The goal of this paper is the derivation and application of a direct characterization of the inverse of the covariance matrix central to portfolio analysis. Such a char- acterization, in terms of a few primitive constructs, provides the basis for new and illuminating expressions for key concepts as the optimal holding of a given risky asset and the slope of the risk-return efficiency frontier faced by the individual investor. The building blocks of the inverse turn out to be the regression coeffi- cients and residual variance obtained by regressing the asset's excess return on the set of excess returns for all other risky assets},
author = {Stephens, Guy},
file = {:C$\backslash$:/Users/wdona/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stephens - 1998 - On the Inverse of the Covariance Matrix in Portfolio Analysis.pdf:pdf},
journal = {The Journal of Finance},
mendeley-groups = {cov{\_}inv{\_}matrix},
number = {5},
pages = {1821--1827},
title = {{On the Inverse of the Covariance Matrix in Portfolio Analysis}},
volume = {53},
year = {1998}
}

@article{Kwan2014,
abstract = {The usefulness of covariance and correlation matrices is well-known in various academic fields. Matrix inversion, if required in an analytical setting, tends to mask the intuition in interpreting the corresponding empirical or experimental results. Drawing on the finance literature in mean-variance portfolio analysis, this paper presents pedagogically a regression-based interpretation of the inverse of the sample covariance matrix. Microsoft ExcelTM plays an important pedagogic role in this paper. The availability of various Excel functions and computational tools for numerical illustrations provides flexibility for instructors in the delivery of the corresponding analytical materials.},
author = {Kwan, C. C. Y.},
file = {:C$\backslash$:/Users/wdona/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwan - 2014 - A Regression-Based Interpretation of the Inverse of the Sample Covariance Matrix.pdf:pdf},
issn = {1448-6156},
journal = {Spreadsheets in Education},
mendeley-groups = {cov{\_}inv{\_}matrix},
number = {1},
title = {{A Regression-Based Interpretation of the Inverse of the Sample Covariance Matrix}},
volume = {7},
year = {2014}
}


@Article{brms,
    title = {Advanced {Bayesian} Multilevel Modeling with the {R} Package {brms}},
    author = {Paul-Christian Bürkner},
    journal = {The R Journal},
    year = {2018},
    volume = {10},
    number = {1},
    pages = {395--411},
    doi = {10.32614/RJ-2018-017},
    encoding = {UTF-8},
  }


 @Manual{shinyapp,
    title = {shiny: Web Application Framework for R},
    author = {Winston Chang and Joe Cheng and JJ Allaire and Yihui Xie and Jonathan McPherson},
    year = {2019},
    note = {R package version 1.4.0},
    url = {https://CRAN.R-project.org/package=shiny},
  }
  
  @Manual{shinydb,
    title = {shinydashboard: Create Dashboards with 'Shiny'},
    author = {Winston Chang and Barbara {Borges Ribeiro}},
    year = {2018},
    note = {R package version 0.7.1},
    url = {https://CRAN.R-project.org/package=shinydashboard},
  }


@article{Rouder2018,
abstract = {In the psychological literature, there are two seemingly different approaches to inference: that from estimation of posterior intervals and that from Bayes factors. We provide an overview of each method and show that a salient difference is the choice of models. The two approaches as commonly practiced can be unified with a certain model specification, now popular in the statistics literature, called spike-and-slab priors. A spike-and-slab prior is a mixture of a null model, the spike, with an effect model, the slab. The estimate of the effect size here is a function of the Bayes factor, showing that estimation and model comparison can be unified. The salient difference is that common Bayes factor approaches provide for privileged consideration of theoretically useful parameter values, such as the value corresponding to the null hypothesis, while estimation approaches do not. Both approaches, either privileging the null or not, are useful depending on the goals of the analyst.},
author = {Rouder, Jeffrey N. and Haaf, Julia M. and Vandekerckhove, Joachim},
doi = {10.3758/s13423-017-1420-7},
file = {:C$\backslash$:/Users/wdona/Dropbox/results Donny{\&}He/10.3758{\%}2Fs13423-017-1420-7.pdf:pdf},
isbn = {1342301714},
issn = {15315320},
journal = {Psychonomic Bulletin and Review},
keywords = {Bayesian inference and parameter estimation,Bayesian statistics,Model selection},
mendeley-groups = {bayes{\_}general},
number = {1},
pages = {102--113},
publisher = {Psychonomic Bulletin {\&} Review},
title = {{Bayesian inference for psychology, part IV: parameter estimation and Bayes factors}},
volume = {25},
year = {2018}
}
